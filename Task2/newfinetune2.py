"""
LLM Fine-tuning for Knowledge Graph Triple Extraction - Case 2

This script fine-tunes a Mistral-7B language model for extracting knowledge graph triples
from news articles using spaCy-filtered subject entities. The model is trained on subjects that were
extracted using spaCy NLP processing and filtered from gold standard triples. Custom prompts
are generated for each subject to teach the model to identify relevant subject-predicate-object
relationships in textual content.

Key Features:
- Uses spaCy-extracted and filtered subjects from gold triples for training
- Implements subject-focused triple extraction with custom prompt engineering
- Employs LoRA (Low-Rank Adaptation) for efficient parameter updates
- Evaluates model performance on predicate-object pair extraction
- Integrates with Weights & Biases for experiment tracking

Training Data: Gold filtered train/test sets with spaCy-processed subject entities
             (generated by newKG21.py preprocessing pipeline)
Model: Mistral-7B-Instruct with 4-bit quantization and LoRA adapters
Evaluation: Precision, Recall, and F1 scores on filtered triple extraction task
Approach: Subject-centric filtering approach using spaCy dependency parsing
"""

import os


os.environ["HF_HOME"] = "/cfs/earth/scratch/adlurmoh/track2/hf_cache"
os.environ["TRANSFORMERS_CACHE"] = "/cfs/earth/scratch/adlurmoh/track2/hf_cache"
os.environ["WANDB_CACHE_DIR"] = "/cfs/earth/scratch/adlurmoh/track2/wandb_cache"
os.environ["TRITON_CACHE_DIR"] = "/cfs/earth/scratch/adlurmoh/track2/triton_cache"

import re
import torch
from unsloth import FastLanguageModel
import pandas as pd
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import Dataset
import json
import ast
import logging
import wandb

# ========= GPU ASSERT ========= #
assert torch.cuda.is_available(), "CUDA not available"
device = torch.device("cuda")
print(f"Using device: {torch.cuda.get_device_name(0)}")

# ========= Login to wandb ========= #
os.environ["WANDB_API_KEY"] = "REMOVED_FOR_SECURITY"  # Replace with your actual key
wandb.init(project="newsKG21-triple-extraction", name="mistral-finetune", config={"epochs": 3})

# ========= Logging Setup ========= #
logging.basicConfig(
    filename="train_log.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def log_print(msg):
    print(msg)
    logging.info(msg)

major_version, minor_version = torch.cuda.get_device_capability()
log_print(f"CUDA compute capability: {major_version}.{minor_version}")

# ========= Load Model ========= #
max_seq_length = 512
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=True,
)

# ========= Apply LoRA Adapters ========= #
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=16,
    lora_dropout=0.0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
    use_rslora=False,
    loftq_config=None,
)

# ========= Load Train/Test Data ========= #
try:
    data_train = pd.read_csv('/cfs/earth/scratch/adlurmoh/track2/case2/gold_filter_train.csv')
    data_test = pd.read_csv('/cfs/earth/scratch/adlurmoh/track2/case2/gold_filter_test.csv')
    log_print(data_train.head())
except FileNotFoundError:
    log_print(" Error: 'train.csv' or 'test.csv' not found.")
    raise

# ========= Format Dataset ========= #
def convert_newskg21_to_alpaca(data):
    formatted = []
    column_name = data.columns[0]
    for index, row in data.iterrows():
        parsed = ast.literal_eval(row[column_name])
        sentence = parsed.get("sentence", "")
        triples = parsed.get("triple", [])
        formatted.append({
            "instruction": "Extract triples from the sentence.",
            "input": sentence,
            "output": str(triples)
        })
    return formatted

# formatted_data = convert_newskg21_to_alpaca(data_train)
# dataset = Dataset.from_list(formatted_data)
dataset_train = Dataset.from_pandas(data_train)
dataset_test = Dataset.from_pandas(data_test)
print('dataset_test',dataset_test)


alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token

def format_prompt(example):
    return {
        "text": alpaca_prompt.format(example["instruction"], example["input"], example["output"]) + EOS_TOKEN
    }

datasetTrain = dataset_train.map(format_prompt)

# ========= Fine-Tuning ========= #
training_args = TrainingArguments(
    output_dir="news_llm_outputs",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=10,
    report_to="wandb",
    run_name="mistral_newsKG21",
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    optim="adamw_8bit",
    lr_scheduler_type="linear",
    # save_steps=50,
    # save_total_limit=2,
    # evaluation_strategy="no",
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=datasetTrain,
    dataset_text_field="text",
    args=training_args,
    packing=False,
    max_seq_length=max_seq_length,
)

trainer.train()

model.save_pretrained("newsKG21_finetuned_model2")
tokenizer.save_pretrained("newsKG21_finetuned_model2")
log_print(" Model and tokenizer saved to 'newsKG21_finetuned_model2'")

# ========= Test Inference ========= #
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="newsKG21_finetuned_model2",
    max_seq_length=512,
    dtype=None,
    load_in_4bit=True,
)
FastLanguageModel.for_inference(model)

# test_df = pd.read_csv("/content/test.csv")
# test_dataset = Dataset.from_pandas(test_df)

def generate_prediction(example):
    prompt = alpaca_prompt.format(example["instruction"], example["input"], "")
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=128)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded.split("### Response:")[-1].strip()

print(" Generating predictions...")
predictions = [generate_prediction(example) for example in dataset_test]
ground_truths = [example["output"] for example in dataset_test]
# print('Test Predictions',predictions)
# print('Test GT:',ground_truths)
# ========= Safe Parse Utility ========= #
# def safe_parse(text):
#     try:
#         # Fix quote issues
#         text = text.replace("’", "'").replace("“", '"').replace("”", '"')
#         text = re.sub(r"(?<=\w)'(?=\w)", r"\\'", text)  # escape single apostrophes
#         text = re.sub(r"'([^']*)'", r'"\1"', text)
#         parsed = ast.literal_eval(text)
#         return set(tuple(t) for t in parsed if isinstance(t, (list, tuple)) and len(t) == 3)
#     except Exception as e:
#         log_print(f"\n Parse error:\n{text}\nError: {e}")
#         return set()

# def safe_parse(text):
#     try:
#         # Clean up common quote issues
#         text = text.replace("’", "'").replace("“", '"').replace("”", '"')
#         text = re.sub(r"(?<!\\)'", r"\\'", text)
#         text = re.sub(r"'([^']*)'", r'"\1"', text)

#         parsed = ast.literal_eval(text)

#         # Handle single triple
#         if isinstance(parsed, list) and len(parsed) == 3 and all(isinstance(x, str) for x in parsed):
#             return {tuple(parsed)}

#         # Handle list of triples
#         return set(tuple(t) for t in parsed if isinstance(t, list) and len(t) == 3)

#     except Exception as e:
#         print(f"\n Final parse error:\n{text}\nError: {e}")
#         return set()


# parsed_preds = [safe_parse(pred) for pred in predictions]
# parsed_truths = [safe_parse(gt) for gt in ground_truths]

# print('Test Predictions',parsed_preds)
# print('Test GT:',parsed_truths)
# # ========= Calculate Metrics on Predicate & Object ========= #

# # Only compare (predicate, object)
# def po_only(triple_set):
#     return set((p, o) for (_, p, o) in triple_set)

# tp = fp = fn = 0
# for pred_set, truth_set in zip(parsed_preds, parsed_truths):
#     pred_po = po_only(pred_set)
#     truth_po = po_only(truth_set)
    
#     print('Test Predictions',pred_set)
#     print('Test GT:',truth_set)
#     tp += len(pred_po & truth_po)
#     fp += len(pred_po - truth_po)
#     fn += len(truth_po - pred_po)
#     print('Test Predictions',pred_po)
#     print('Test GT:',truth_po)
###################################################################################################


import ast
import re

# ========= Fixed Parser ========= #
def safe_parse(text):
    try:
        # Normalize smart quotes
        text = text.replace("’", "'").replace("“", '"').replace("”", '"')

        # Try parsing string
        parsed = ast.literal_eval(text)

        # CASE 1: A single triple
        if isinstance(parsed, list) and len(parsed) == 3 and all(isinstance(x, str) for x in parsed):
            return {tuple(parsed)}

        # CASE 2: List of triples
        if isinstance(parsed, list) and all(isinstance(t, list) and len(t) == 3 for t in parsed):
            return set(tuple(t) for t in parsed)

        return set()

    except Exception as e:
        print(f"\n Final parse error:\n{text}\nError: {e}")
        return set()

# ========= Apply Safe Parsing ========= #
parsed_preds = [safe_parse(pred) for pred in predictions]
parsed_truths = [safe_parse(gt) for gt in ground_truths]

# print('Test Predictions',parsed_preds)
# print('Test GT:',parsed_truths)
# ========= Evaluate Predicate + Object Only ========= #
def po_only(triple_set):
    return set((p, o) for (_, p, o) in triple_set)

tp = fp = fn = 0
print("\n--- Evaluation Samples ---")
for idx, (pred_set, truth_set) in enumerate(zip(parsed_preds, parsed_truths)):
    pred_po = po_only(pred_set)
    truth_po = po_only(truth_set)

    tp += len(pred_po & truth_po)
    fp += len(pred_po - truth_po)
    fn += len(truth_po - pred_po)

    # # Optional logging
    # print('Predictions',parsed_preds)
    # print('GT:',parsed_truths)
    # print(f"\nSample {idx + 1}")
    # print("  GT P-O:", truth_po)
    # print("  PR P-O:", pred_po)

# ========= Metrics ========= #
precision = tp / (tp + fp + 1e-8)
recall    = tp / (tp + fn + 1e-8)
f1        = 2 * precision * recall / (precision + recall + 1e-8)

print("\n=== Final Evaluation Metrics (on Predicate + Object Only) ===")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

####################################################################################################
# tp = fp = fn = 0
# for idx, (pred_set, truth_set) in enumerate(zip(parsed_preds, parsed_truths)):
#     # Use only predicate-object pairs
#     pred_po = set((p, o) for (_, p, o) in pred_set)
#     truth_po = set((p, o) for (_, p, o) in truth_set)

#     # Track metrics
#     tp += len(pred_po & truth_po)
#     fp += len(pred_po - truth_po)
#     fn += len(truth_po - pred_po)

#     #  Match log
#     if pred_po & truth_po:
#         log_print(f"\n Match found for sample {idx}")
#         log_print(f"PREDICTED PO: {pred_po}")
#         log_print(f"GROUND TRUTH PO: {truth_po}")

#     #  Mismatch log
#     if pred_po != truth_po:
#         log_print(f"\n Mismatch at sample {idx}")
#         log_print(f"PREDICTED PO: {pred_po}")
#         log_print(f"GROUND TRUTH PO: {truth_po}")

#     if pred_po != truth_po:
#         print(f"\nMismatch:")
#         print(f"  Pred : {pred_po}")
#         print(f"  Truth: {truth_po}")


# # ========= Print Final Evaluation ========= #
# precision = tp / (tp + fp + 1e-8)
# recall    = tp / (tp + fn + 1e-8)
# f1        = 2 * precision * recall / (precision + recall + 1e-8)

# print("\n Evaluation Metrics (on predicate + object only):")
# print(f"Precision: {precision:.4f}")
# print(f" Recall:    {recall:.4f}")
# print(f" F1 Score:  {f1:.4f}")

wandb.log({"precision": precision, "recall": recall, "f1": f1})

# print("Raw prediction sample:", predictions[0])
# print("Parsed prediction set:", parsed_preds[0])
# print("Parsed truth set:", parsed_truths[0])


# sample = 0
# print("Instruction:", dataset_test[sample]["instruction"])
# print("Input:", dataset_test[sample]["input"])
# print("Ground truth:", ground_truths[sample])
# print("Prediction:", predictions[sample])


# ========= Save Predictions ========= #
with open("finetune2.jsonl", "w") as f:
    for example, pred in zip(dataset_test, predictions):
        f.write(json.dumps({
            "instruction": example["instruction"],
            "input": example["input"],
            "true_output": example["output"],
            "predicted_output": pred
        }) + "\n")

log_print(" Predictions saved to finetune2.jsonl")

wandb.finish()
